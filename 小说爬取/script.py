import requests
from bs4 import BeautifulSoup
import time
import os
import re
import subprocess
import json
import uuid
from urllib.parse import urljoin
from html import unescape

config = {
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 13) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.230 Mobile Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml;q=0.9,image/webp,*/*;q=0.8'
    },
    'save_path': '/storage/emulated/0/Download/novels',
    'request_interval': 2,
    'max_retries': 5,
    'termux_notify': True
}

class BiqugeDownloader:
    def __init__(self):
        self.start_url = None
        self.current_url = None
        self.session = requests.Session()
        self.session.headers.update(config['headers'])
        self.chapter_count = 0
        self.novel_name = None
        self.merge_action = 0  # 0:åˆå¹¶åˆ é™¤ 1:ä»…ä¿å­˜ 2:åˆå¹¶ä¿ç•™
        os.makedirs(config['save_path'], exist_ok=True)
        self.is_termux = 'com.termux' in os.getcwd()

    def show_notification(self, title, message):
        if self.is_termux and config['termux_notify']:
            try:
                subprocess.run([
                    'termux-notification',
                    '--title', title,
                    '--content', message,
                    '--led-color', 'FF00FF00'
                ], check=True)
            except (FileNotFoundError, subprocess.CalledProcessError) as e:
                print(f"é€šçŸ¥å‘é€å¤±è´¥: {str(e)}")

    def termux_dialog(self, dialog_type, title, values=None, default_input=""):
        try:
            cmd = ['termux-dialog']
            if dialog_type == "text":
                cmd += ['text', '-t', title, '-i', default_input]
            elif dialog_type == "radio":
                cmd += ['radio', '-v', ','.join(values), '-t', title]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=45
            )
            raw_output = result.stdout.strip()
           # print(f"[DEBUG] Dialog Raw Output: {raw_output}")

            # å¢å¼ºJSONè§£æ
            data = {}
            try:
                data = json.loads(raw_output)
            except json.JSONDecodeError:
                code_match = re.search(r'"code"\s*:\s*(-?\d+)', raw_output)
                text_match = re.search(r'"text"\s*:\s*"?(.*?)"?[\},]', raw_output)
                data = {
                    "code": int(code_match.group(1)) if code_match else -2,
                    "text": text_match.group(1).strip() if text_match else ""
                }

           # print(f"[DEBUG] Parsed Data: {data}")
            return data if data.get('text') else None

        except Exception as e:
            print(f"å¯¹è¯æ¡†å¼‚å¸¸: {str(e)}")
            return None

    def get_user_input(self):
        # è·å–èµ·å§‹URL
        url_dialog = self.termux_dialog(
            "text", 
            "ğŸ“– è¯·è¾“å…¥å°è¯´èµ·å§‹ç½‘å€",
            default_input="https://www.biquge.com/book/"
        )
        
        if url_dialog and url_dialog.get('text', '').strip():
            self.start_url = self.current_url = url_dialog['text'].strip()
            #print(f"è·å–åˆ°æœ‰æ•ˆURL: {self.start_url}")
            time.sleep(0.5)
        else:
            self.show_notification("è¾“å…¥å–æ¶ˆ", "æœªæä¾›èµ·å§‹ç½‘å€")
            return False
        
        # è·å–æ“ä½œé€‰é¡¹
        action_options = [
            "ğŸš€ åˆå¹¶ååˆ é™¤ç« èŠ‚", 
            "ğŸ’¾ ä»…ä¿å­˜ç« èŠ‚æ–‡ä»¶", 
            "ğŸ“š åˆå¹¶ä¿ç•™ç« èŠ‚"
        ]
        action_dialog = self.termux_dialog(
            "radio",
            "ğŸ› ï¸ è¯·é€‰æ‹©æ“ä½œæ¨¡å¼",
            values=action_options
        )
        
        # å¤„ç†æ“ä½œé€‰æ‹©
        if action_dialog and action_dialog.get('text'):
            try:
                selected_index = int(action_dialog['text'])
                if 0 <= selected_index < len(action_options):
                    self.merge_action = selected_index
                else:
                    raise ValueError("æ— æ•ˆçš„é€‰é¡¹ç´¢å¼•")
            except Exception as e:
                print(f"é€‰é¡¹è§£æé”™è¯¯: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼")
                self.merge_action = 0
        else:
            print("ä½¿ç”¨é»˜è®¤åˆå¹¶æ¨¡å¼")
            self.merge_action = 0
        
        return True

    def sanitize_filename(self, name):
        return re.sub(r'[\\/:*?"<>|]', '', name).strip()[:80]

    def extract_novel_name(self, title_text):
        patterns = [
            r'(.*?)æœ€æ–°ç« èŠ‚', 
            r'(.*?)\s*-\s*',
            r'æœ€æ–°ç« ï¼š(.*?)\s*\|'
        ]
        for pattern in patterns:
            match = re.search(pattern, title_text)
            if match:
                return self.sanitize_filename(match.group(1))
        parts = [p.strip() for p in title_text.split('-')]
        return self.sanitize_filename(parts[0]) if len(parts) > 1 else f"æ— åå°è¯´_{uuid.uuid4().hex[:6]}"

    def get_page_content(self, url):
        for retry in range(config['max_retries'])exit:
            try:
                response = self.session.get(url, timeout=15)
                response.encoding = 'utf-8'
                response.raise_for_status()
                
                if not self.novel_name:
                    soup = BeautifulSoup(response.text, 'lxml')
                    title_tag = soup.find('h1') or soup.find('title')
                    if title_tag:
                        self.novel_name = self.extract_novel_name(title_tag.get_text().strip())
                        self.show_notification("å¼€å§‹ä¸‹è½½", f"ã€Š{self.novel_name}ã€‹")
                
                return response.text
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥({retry+1}/{config['max_retries']}): {str(e)}")
                time.sleep(2)
        return None

    def clean_content(self, text):
        text = unescape(text)
        replacements = {
            r'&nbsp;|\u3000': ' ',
            r'å†…å®¹æœªå®Œ[^\n]+': '',
            r'è¿˜ä¸èµ¶å¿«æ¥ä½“éªŒï¼+': '',
            r'è¯·æ”¶è—æœ¬ç«™ï¼šhttps://www\.biquge\d+\.comã€‚\s*ç¬”è¶£é˜æ‰‹æœºç‰ˆï¼šhttps://m\.biquge\d+\.com': ''
        }
        for pattern, repl in replacements.items():
            text = re.sub(pattern, repl, text)
        return '\n\n'.join([line.strip() for line in text.split('\n') if line.strip()])

    def parse_page(self, html):
        soup = BeautifulSoup(html, 'lxml')
        
        # æå–æ ‡é¢˜
        title_tag = soup.find('h1') or soup.find('title')
        title_text = title_tag.get_text().strip() if title_tag else "æœªçŸ¥ç« èŠ‚"
        chapter_title = re.sub(r'^.*?(ç¬¬[^ç« ]+ç« )', r'\1', title_text.split('-')[0].strip())
        
        # æå–å†…å®¹
        content_div = soup.find('div', id='novelcontent')
        if not content_div:
            raise ValueError("æœªæ‰¾åˆ°ç« èŠ‚å†…å®¹")
        
        # æ¸…ç†å¹¿å‘Š
        for tag in content_div.find_all(['script', 'font', 'div', 'a']):
            tag.decompose()
        
        # å¤„ç†åˆ†é¡µ
        next_url = None
        page_nav = soup.find('div', class_='page_chapter')
        if page_nav:
            next_btn = page_nav.find('a', string=re.compile(r'ä¸‹ä¸€é |ä¸‹ä¸€é¡µ|ä¸‹ä¸€ç« '))
            if next_btn and next_btn.get('href'):
                next_url = urljoin(self.current_url, next_btn['href'])
        
        return {
            'title': self.sanitize_filename(chapter_title),
            'content': self.clean_content(content_div.get_text(separator='\n')),
            'next_url': next_url if next_url != self.current_url else None
        }

    def save_chapter(self, title, content):
        self.chapter_count += 1
        filename = os.path.join(config['save_path'], f"{self.chapter_count:04d}_{title}.txt")
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"ã€{title}ã€‘\n\n{content}\n")
            print(f"âœ… å·²ä¿å­˜: {filename}")
            return True
        except PermissionError:
            print("âŒ æƒé™ä¸è¶³ï¼Œè¯·æ‰§è¡Œï¼štermux-setup-storage")
            self.show_notification("é”™è¯¯", "éœ€è¦å­˜å‚¨æƒé™")
            exit(1)
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
            return False

    def merge_chapters(self):
        chapter_files = sorted(
            [f for f in os.listdir(config['save_path']) 
             if re.match(r'^\d{4}_', f) and f.endswith('.txt')],
            key=lambda x: int(x.split('_')[0])
        )
        
        if not chapter_files:
            print("âš ï¸ æ²¡æœ‰å¯åˆå¹¶çš„ç« èŠ‚")
            return None
        
        merged_name = f"{self.novel_name}.txt" if self.novel_name else f"åˆå¹¶å°è¯´_{time.strftime('%Y%m%d%H%M')}.txt"
        merged_path = os.path.join(config['save_path'], merged_name)
        
        try:
            with open(merged_path, 'w', encoding='utf-8') as mf:
                total = len(chapter_files)
                for idx, cf in enumerate(chapter_files, 1):
                    cf_path = os.path.join(config['save_path'], cf)
                    with open(cf_path, 'r', encoding='utf-8') as sf:
                        mf.write(sf.read() + '\n\n')
                    print(f"ğŸ“¦ åˆå¹¶è¿›åº¦: {idx}/{total}")
            
            print(f"âœ… åˆå¹¶å®Œæˆ: {merged_path}")
            self.show_notification("åˆå¹¶æˆåŠŸ", os.path.basename(merged_path))
            return merged_path
        except Exception as e:
            print(f"âŒ åˆå¹¶å¤±è´¥: {str(e)}")
            self.show_notification("åˆå¹¶å¤±è´¥", str(e))
            return None

    def merge_and_clean(self):
        if merged_file := self.merge_chapters():
            chapter_files = [f for f in os.listdir(config['save_path']) 
                           if re.match(r'^\d{4}_', f) and f.endswith('.txt')]
            for cf in chapter_files:
                try:
                    os.remove(os.path.join(config['save_path'], cf))
                except Exception as e:
                    print(f"âŒ åˆ é™¤å¤±è´¥: {cf} - {str(e)}")
            print(f"ğŸ—‘ï¸ å·²æ¸…ç† {len(chapter_files)} ä¸ªç« èŠ‚æ–‡ä»¶")

    def download_all(self):
        if not self.get_user_input():
            return

        print("ğŸ å¼€å§‹ä¸‹è½½ï¼ŒæŒ‰Ctrl+Cåœæ­¢")
        start_time = time.time()
        try:
            while self.current_url:
                print(f"\nğŸ“¡ æŠ“å–: {self.current_url}")
                html = self.get_page_content(self.current_url)
                if not html:
                    print("ğŸš¨ è·å–é¡µé¢å¤±è´¥")
                    break
                
                try:
                    data = self.parse_page(html)
                    self.save_chapter(data['title'], data['content'])
                    self.current_url = data['next_url']
                except Exception as e:
                    print(f"âŒ è§£æé”™è¯¯: {str(e)}")
                    break
                
                time.sleep(config['request_interval'])
            
            # æ‰§è¡Œåˆå¹¶æ“ä½œ
            if self.merge_action == 0:
                self.merge_and_clean()
            elif self.merge_action == 2:
                self.merge_chapters()
            
            time_cost = time.time() - start_time
            stats = f"è€—æ—¶: {time_cost:.1f}ç§’\nç« èŠ‚: {self.chapter_count}ç« "
            self.show_notification("ä¸‹è½½å®Œæˆ", stats)
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
            self.show_notification("ä¸‹è½½ä¸­æ–­", f"å·²ä¿å­˜ {self.chapter_count} ç« ")

if __name__ == "__main__":
    downloader = BiqugeDownloader()
    downloader.download_all()
