import requests
from bs4 import BeautifulSoup
import time
import os
import re
import subprocess
import argparse

config = {
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 10) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.230 Mobile Safari/537.36'
    },
    'save_path': '/storage/emulated/0/Download/novels',
    'request_interval': 3,
    'max_retries': 5,
    'termux_notify': True
}

class TermuxNovelDownloader:
    def __init__(self, start_url=None):
        self.start_url = start_url
        self.current_url = start_url
        self.session = requests.Session()
        self.session.headers.update(config['headers'])
        self.chapter_count = 0
        os.makedirs(config['save_path'], exist_ok=True)
        self.is_termux = 'com.termux' in os.getcwd()

    def show_notification(self, title, message):
        if self.is_termux and config['termux_notify']:
            try:
                subprocess.run([
                    'termux-notification',
                    '--title', title,
                    '--content', message
                ])
            except FileNotFoundError:
                print("æœªæ‰¾åˆ°termux-notificationï¼Œè¯·å…ˆå®‰è£…termux-api")

    def sanitize_filename(self, name):
        return re.sub(r'[\\/:*?"<>|]', '', name).strip()

    def get_page_content(self, url):
        for retry in range(config['max_retries']):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                return response.text
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥({retry+1}/{config['max_retries']}): {str(e)}")
                time.sleep(2)
        return None

    def parse_page(self, html):
        soup = BeautifulSoup(html, 'lxml')
        title_tag = soup.find('title')
        chapter_title = title_tag.text.split('_')[0] if title_tag else "æœªçŸ¥ç« èŠ‚"
        clean_title = self.sanitize_filename(chapter_title)
        
        content_div = soup.find('div', {'id': 'chaptercontent'})
        if not content_div:
            raise ValueError("æœªæ‰¾åˆ°ç« èŠ‚å†…å®¹")
        
        paragraphs = []
        for p in content_div.find_all('p'):
            text = p.text.strip()
            if text and not text.startswith('<!--'):
                paragraphs.append(text)
        
        next_link = soup.find('a', {'id': 'pt_next'})
        next_url = None
        if next_link and 'href' in next_link.attrs and 'æ²¡æœ‰äº†' not in next_link.text:
            next_url = requests.compat.urljoin(self.current_url, next_link['href'])
        
        return {
            'title': clean_title,
            'content': '\n\n'.join(paragraphs),
            'next_url': next_url
        }

    def save_chapter(self, title, content):
        self.chapter_count += 1
        filename = os.path.join(config['save_path'], f"{self.chapter_count:03d}_{title}.txt")
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ… å·²ä¿å­˜: {filename}")
        except PermissionError:
            print(f"âŒ æƒé™ä¸è¶³ï¼Œè¯·è¿è¡Œ: termux-setup-storage å¹¶å…è®¸æ–‡ä»¶è®¿é—®")
            self.show_notification("ä¸‹è½½å¤±è´¥", "å­˜å‚¨æƒé™ä¸è¶³")
            exit(1)

    def merge_chapters(self):
        chapter_files = sorted(
            [f for f in os.listdir(config['save_path']) 
             if re.match(r'^\d{3}_', f) and f.endswith('.txt')],
            key=lambda x: int(x.split('_')[0])
        )
        
        if not chapter_files:
            print("âš ï¸ æ²¡æœ‰å¯åˆå¹¶çš„ç« èŠ‚æ–‡ä»¶")
            return
        
        merged_file = os.path.join(config['save_path'], "merged_novel.txt")
        try:
            files_to_delete = [os.path.join(config['save_path'], cf) for cf in chapter_files]
            
            with open(merged_file, 'w', encoding='utf-8') as mf:
                for cf in chapter_files:
                    with open(os.path.join(config['save_path'], cf), 'r', encoding='utf-8') as sf:
                        mf.write(sf.read() + '\n\n')
            
            deleted_count = 0
            for fpath in files_to_delete:
                try:
                    os.remove(fpath)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {os.path.basename(fpath)}")
                except Exception as e:
                    print(f"âŒ åˆ é™¤å¤±è´¥ {os.path.basename(fpath)}: {str(e)}")
            
            print(f"âœ… åˆå¹¶å®Œæˆ: {merged_file}")
            print(f"ğŸ—‘ï¸ å·²æ¸…ç† {deleted_count}/{len(chapter_files)} ä¸ªç« èŠ‚æ–‡ä»¶")
            self.show_notification("åˆå¹¶å®Œæˆ", 
                                f"æœ€ç»ˆæ–‡ä»¶: {merged_file}\næ¸…ç†æ–‡ä»¶: {deleted_count}ä¸ª")
        except Exception as e:
            print(f"âŒ åˆå¹¶å¤±è´¥: {str(e)}")
            self.show_notification("åˆå¹¶å¤±è´¥", str(e))

    def download_all(self, merge_after=False):
        print("ğŸ å¼€å§‹ä¸‹è½½ï¼ŒæŒ‰Ctrl+Cåœæ­¢")
        try:
            while self.current_url:
                print(f"ğŸ“– æ­£åœ¨ä¸‹è½½: {self.current_url}")
                html = self.get_page_content(self.current_url)
                if not html:
                    print("ğŸš¨ é¡µé¢åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æœ¬ç« èŠ‚")
                    self.current_url = None
                    break
                
                try:
                    data = self.parse_page(html)
                    self.save_chapter(data['title'], data['content'])
                    self.current_url = data['next_url']
                except Exception as e:
                    print(f"ğŸš¨ è§£æå¤±è´¥: {str(e)}")
                    self.show_notification("ä¸‹è½½é”™è¯¯", str(e))
                    break
                
                time.sleep(config['request_interval'])
            
            print("ğŸ‰ æ‰€æœ‰ç« èŠ‚ä¸‹è½½å®Œæˆï¼")
            if merge_after:
                self.merge_chapters()
            self.show_notification("ä¸‹è½½å®Œæˆ", f"æ–‡ä»¶ä¿å­˜åœ¨ï¼š{config['save_path']}")
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
            self.show_notification("ä¸‹è½½ä¸­æ–­", "ç”¨æˆ·ä¸»åŠ¨åœæ­¢")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Termuxå°è¯´ä¸‹è½½å·¥å…·')
    parser.add_argument('url', nargs='?', help='å°è¯´èµ·å§‹ç« èŠ‚çš„URL')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('--download-only', action='store_true', help='ä»…ä¸‹è½½ç« èŠ‚ä¸åˆå¹¶')
    group.add_argument('--merge-only', action='store_true', help='ä»…åˆå¹¶å·²ä¸‹è½½ç« èŠ‚')
    
    args = parser.parse_args()

    if args.merge_only:
        downloader = TermuxNovelDownloader()
        downloader.merge_chapters()
    elif args.download_only or args.url:
        if not args.url:
            parser.error("ä¸‹è½½æ¨¡å¼éœ€è¦æä¾›èµ·å§‹URL")
        downloader = TermuxNovelDownloader(args.url)
        downloader.download_all(merge_after=not args.download_only)
    else:
        parser.print_help()
        print("\nâš ï¸ è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼ï¼š")
        print("1. ä¸‹è½½å¹¶åˆå¹¶: python script.py <èµ·å§‹URL>")
        print("2. ä»…ä¸‹è½½: python script.py <èµ·å§‹URL> --download-only")
        print("3. ä»…åˆå¹¶: python script.py --merge-only")
