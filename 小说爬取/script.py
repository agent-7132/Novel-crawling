import requests
from bs4 import BeautifulSoup
import time
import os
import re
import subprocess
import argparse
import uuid
from urllib.parse import urljoin
from html import unescape

config = {
    'headers': {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 13) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.230 Mobile Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml;q=0.9,image/webp,*/*;q=0.8'
    },
    'save_path': '/storage/emulated/0/Download/novels',
    'request_interval': 2,
    'max_retries': 5,
    'termux_notify': True
}

class BiqugeDownloader:
    def __init__(self, start_url=None):
        self.start_url = start_url
        self.current_url = start_url
        self.session = requests.Session()
        self.session.headers.update(config['headers'])
        self.chapter_count = 0
        self.novel_name = None  # æ–°å¢å°è¯´åç§°å­˜å‚¨
        os.makedirs(config['save_path'], exist_ok=True)
        self.is_termux = 'com.termux' in os.getcwd()

    def show_notification(self, title, message):
        if self.is_termux and config['termux_notify']:
            try:
                subprocess.run([
                    'termux-notification',
                    '--title', title,
                    '--content', message,
                    '--led-color', 'FF00FF00'
                ])
            except FileNotFoundError:
                print("æœªæ‰¾åˆ°termux-notificationï¼Œè¯·å…ˆå®‰è£…termux-api")

    def sanitize_filename(self, name):
        return re.sub(r'[\\/:*?"<>|]', '', name).strip()[:80]

    def extract_novel_name(self, title_text):
        """ä»ç½‘é¡µæ ‡é¢˜ä¸­æå–å°è¯´åç§°"""
        patterns = [
            r'(.*?)æœ€æ–°ç« èŠ‚',      # åŒ¹é… "å°è¯´åæœ€æ–°ç« èŠ‚"
            r'(.*?)\s*-\s*',      # åŒ¹é… "å°è¯´å - ç½‘ç«™å"
            r'æœ€æ–°ç« ï¼š(.*?)\s*\|' # åŒ¹é… "æœ€æ–°ç« ï¼šå°è¯´å |"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title_text)
            if match:
                return self.sanitize_filename(match.group(1))
        
        # å¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½ä¸åŒ¹é…ï¼Œå°è¯•åˆ†å‰²çŸ­æ¨ªçº¿
        parts = [p.strip() for p in title_text.split('-')]
        if len(parts) > 1:
            return self.sanitize_filename(parts[0])
        
        return f"æ— åå°è¯´_{uuid.uuid4().hex[:6]}"  # æœ€ç»ˆå›é€€æ–¹æ¡ˆ

    def get_page_content(self, url):
        for retry in range(config['max_retries']):
            try:
                response = self.session.get(url, timeout=15)
                response.encoding = 'utf-8'
                response.raise_for_status()
                
                # é¦–æ¬¡è¯·æ±‚æ—¶è§£æå°è¯´åç§°
                if not self.novel_name and retry == 0:
                    soup = BeautifulSoup(response.text, 'lxml')
                    title_tag = soup.find('h1') or soup.find('title')
                    if title_tag:
                        title_text = title_tag.get_text().strip()
                        self.novel_name = self.extract_novel_name(title_text)
                
                return response.text
            except Exception as e:
                print(f"è¯·æ±‚å¤±è´¥({retry+1}/{config['max_retries']}): {str(e)}")
                time.sleep(2)
        return None

    def clean_content(self, text):
        text = unescape(text)
        text = re.sub(r'&nbsp;|\u3000', ' ', text)
        text = re.sub(r'å†…å®¹æœªå®Œï¼Œ[^\n]+', '', text)
        text = re.sub(r'è¿˜ä¸èµ¶å¿«æ¥ä½“éªŒï¼+', '', text)
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return '\n\n'.join(lines)

    def parse_page(self, html):
        soup = BeautifulSoup(html, 'lxml')
        
        title_tag = soup.find('h1') or soup.find('title')
        title_text = title_tag.get_text().strip() if title_tag else "æœªçŸ¥ç« èŠ‚"
        
        # æå–ç« èŠ‚æ ‡é¢˜ï¼ˆæ’é™¤å°è¯´åï¼‰
        chapter_title = title_text.split('-')[0].strip()
        chapter_title = re.sub(r'^.*?ç« ', '', chapter_title).strip()  # å»é™¤å¯èƒ½çš„å°è¯´åå‰ç¼€
        
        content_div = soup.find('div', {'id': 'novelcontent'})
        if not content_div:
            raise ValueError("æœªæ‰¾åˆ°ç« èŠ‚å†…å®¹")
        
        for ad in content_div.find_all(['script', 'font', 'div']):
            ad.decompose()
        
        raw_text = content_div.get_text(separator='\n')
        cleaned_content = self.clean_content(raw_text)
        
        next_url = None
        page_nav = soup.find('div', {'class': 'page_chapter'})
        if page_nav:
            next_btn = page_nav.find('a', string=re.compile(r'ä¸‹ä¸€é |ä¸‹ä¸€é¡µ|ä¸‹ä¸€ç« '))
            if next_btn and 'href' in next_btn.attrs:
                next_url = urljoin(self.current_url, next_btn['href'])
        
        return {
            'title': self.sanitize_filename(chapter_title),
            'content': cleaned_content,
            'next_url': next_url if next_url != self.current_url else None
        }

    def save_chapter(self, title, content):
        self.chapter_count += 1
        filename = os.path.join(config['save_path'], f"{self.chapter_count:04d}_{title}.txt")
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"ã€{title}ã€‘\n\n{content}\n")
            print(f"âœ… å·²ä¿å­˜: {filename}")
            return True
        except PermissionError:
            print(f"âŒ æƒé™ä¸è¶³ï¼Œè¯·è¿è¡Œ: termux-setup-storage å¹¶å…è®¸æ–‡ä»¶è®¿é—®")
            self.show_notification("ä¸‹è½½å¤±è´¥", "å­˜å‚¨æƒé™ä¸è¶³")
            exit(1)
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
            return False

    def merge_chapters(self):
        chapter_files = sorted(
            [f for f in os.listdir(config['save_path']) 
             if re.match(r'^\d{4}_', f) and f.endswith('.txt')],
            key=lambda x: int(x.split('_')[0])
        )
        
        if not chapter_files:
            print("âš ï¸ æ²¡æœ‰å¯åˆå¹¶çš„ç« èŠ‚æ–‡ä»¶")
            return
        
        # ç”Ÿæˆåˆå¹¶æ–‡ä»¶å
        if self.novel_name:
            base_name = self.novel_name
        else:
            base_name = f"å°è¯´_{time.strftime('%Y%m%d%H%M%S')}"
        merged_file = os.path.join(config['save_path'], f"{base_name}.txt")
        
        try:
            with open(merged_file, 'w', encoding='utf-8') as mf:
                total = len(chapter_files)
                for i, cf in enumerate(chapter_files, 1):
                    path = os.path.join(config['save_path'], cf)
                    with open(path, 'r', encoding='utf-8') as sf:
                        mf.write(sf.read() + '\n\n')
                    print(f"ğŸ“– åˆå¹¶è¿›åº¦: {i}/{total} - {cf}")
            
            deleted_count = 0
            for cf in chapter_files:
                file_path = os.path.join(config['save_path'], cf)
                try:
                    os.remove(file_path)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {cf}")
                except Exception as e:
                    print(f"âŒ åˆ é™¤æ–‡ä»¶ {cf} å¤±è´¥: {str(e)}")
            
            print(f"âœ… åˆå¹¶å®Œæˆ: {merged_file}")
            self.show_notification("åˆå¹¶å®Œæˆ", f"æ–‡ä»¶å·²ä¿å­˜ä¸º: {os.path.basename(merged_file)}")
            return merged_file
        except Exception as e:
            print(f"âŒ åˆå¹¶å¤±è´¥: {str(e)}")
            self.show_notification("åˆå¹¶å¤±è´¥", str(e))
            return None

    def download_all(self, merge_after=False):
        print("ğŸ å¼€å§‹ä¸‹è½½ï¼ŒæŒ‰Ctrl+Cåœæ­¢")
        try:
            while self.current_url:
                print(f"ğŸ“¡ æ­£åœ¨æŠ“å–: {self.current_url}")
                html = self.get_page_content(self.current_url)
                if not html:
                    print("ğŸš¨ é¡µé¢åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æœ¬ç« èŠ‚")
                    break
                
                try:
                    data = self.parse_page(html)
                    if self.save_chapter(data['title'], data['content']):
                        self.current_url = data['next_url']
                    else:
                        break
                except Exception as e:
                    print(f"ğŸš¨ è§£æå¤±è´¥: {str(e)}")
                    self.show_notification("è§£æé”™è¯¯", str(e))
                    break
                
                time.sleep(config['request_interval'])
            
            print("ğŸ‰ ä¸‹è½½ä»»åŠ¡å®Œæˆ")
            if merge_after:
                self.merge_chapters()
            self.show_notification("ä¸‹è½½å®Œæˆ", f"æ–‡ä»¶ä¿å­˜åœ¨ï¼š{config['save_path']}")
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
            self.show_notification("ä¸‹è½½ä¸­æ–­", "å·²ä¿å­˜å·²ä¸‹è½½ç« èŠ‚")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='ç¬”è¶£é˜å°è¯´ä¸‹è½½å™¨ - Termuxç‰ˆ')
    parser.add_argument('url', nargs='?', help='èµ·å§‹ç« èŠ‚URL')
    group = parser.add_mutually_exclusive_group()
    group.add_argument('-d', '--download-only', action='store_true', help='ä»…ä¸‹è½½ä¸åˆå¹¶')
    group.add_argument('-m', '--merge-only', action='store_true', help='ä»…åˆå¹¶å·²ä¸‹è½½æ–‡ä»¶')
    
    args = parser.parse_args()

    if args.merge_only:
        dl = BiqugeDownloader()
        dl.merge_chapters()
    elif args.download_only or args.url:
        if not args.url:
            parser.error("éœ€è¦æä¾›èµ·å§‹URL")
        dl = BiqugeDownloader(args.url)
        dl.download_all(merge_after=not args.download_only)
    else:
        parser.print_help()
        print("\nç¤ºä¾‹ç”¨æ³•:")
        print("  å®Œæ•´ä¸‹è½½: python novel_dl.py https://m.biquguaxs.com/311/311656/48179016.html")
        print("  ä»…ä¸‹è½½:   python novel_dl.py URL --download-only")
        print("  ä»…åˆå¹¶:   python novel_dl.py --merge-only")
